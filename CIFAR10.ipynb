{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAiHRV8U7JMyHt3njMlixs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinchacoffee/CIFAR10_practice/blob/resnet-ver/CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Or-gRg2GCAq"
      },
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8cMzeW9LCEd",
        "outputId": "38e6a2ff-1be9-475f-d263-30089e4cfddc"
      },
      "source": [
        "random_seed=1\n",
        "train_batch_size=100\n",
        "test_batch_size=1000\n",
        "n_epochs=1\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f38e7cb0270>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRfv87J3t31x",
        "outputId": "8dbf54db-8c7f-4d10-8263-ef421f01f91c"
      },
      "source": [
        "train_norm_load = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR10('/files/', train = True,\n",
        "                                 transform = torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor()\n",
        "                                            ]),\n",
        "                                            download = True),\n",
        "                            batch_size=train_batch_size, shuffle = True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "F7PlIhXwdDHq",
        "outputId": "b6a8af09-ce3d-4c40-b29e-1186de364b08"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig2=plt.figure()\n",
        "for x,y in train_norm_load:\n",
        "  xx=torch.movedim(x[0], (0,1,2), (2,0,1))\n",
        "  plt.imshow(xx)\n",
        "  break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmeYd-gpuE8F",
        "outputId": "74b0a426-abcf-4186-973a-1fe9c90eb71e"
      },
      "source": [
        "RGB_sum = torch.zeros(3)\n",
        "RGB_sum_sq = torch.zeros(3)\n",
        "for x,y in train_norm_load:\n",
        "  RGB_sum += torch.mean(x, dim=(0,2,3))#*x.shape[0]\n",
        "  RGB_sum_sq += torch.mean(x*x, dim=(0,2,3))#*x.shape[0]\n",
        "RGB_mean = RGB_sum / len(train_norm_load)\n",
        "RGB_std  = (RGB_sum_sq / len(train_norm_load) - RGB_mean ** 2)**0.5\n",
        "print(RGB_mean,RGB_std)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2470, 0.2435, 0.2616])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9WEbV97LYQw",
        "outputId": "d058b76f-65a0-4bee-ba62-59e73559af9e"
      },
      "source": [
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR10('/files/', train = True,\n",
        "                                 transform = torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize(\n",
        "                                                RGB_mean, RGB_std )\n",
        "                                            ]),\n",
        "                                            download = True),\n",
        "                            batch_size=train_batch_size, shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR10('/files/', train = False,\n",
        "                                 transform = torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize(\n",
        "                                                (0.5, 0.5, 0.5), (0.2, 0.2, 0.2) )\n",
        "                                            ]), #why not working when without enter?\n",
        "                                            download = True),\n",
        "                            batch_size=test_batch_size, shuffle = True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEI-6lKTgwpZ",
        "outputId": "2d7ef6f3-1667-403e-ee25-69e7a7b90c51"
      },
      "source": [
        "RGB_sum = torch.zeros(3)\n",
        "RGB_sum_sq = torch.zeros(3)\n",
        "for x,y in train_loader:\n",
        "  RGB_sum += torch.mean(x, dim=(0,2,3))#*x.shape[0]\n",
        "  RGB_sum_sq += torch.mean(x*x, dim=(0,2,3))#*x.shape[0]\n",
        "RGB_mean = RGB_sum / len(train_loader)\n",
        "RGB_std  = (RGB_sum_sq / len(train_loader) - RGB_mean ** 2)**0.5\n",
        "print(RGB_mean,RGB_std)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-7.9392e-07,  3.6231e-07, -5.1742e-07]) tensor([1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "aQD7sbFPUIPu",
        "outputId": "75270d21-76ea-4169-deed-9cbbd0a5029a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig1=plt.figure()\n",
        "for x,y in train_loader:\n",
        "  xx=torch.movedim(x[0], (0,1,2), (2,0,1))\n",
        "  plt.imshow(xx)\n",
        "  break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8ViXq8Wtdy"
      },
      "source": [
        "import torch.nn\n",
        "import torch.nn.functional\n",
        "import torch.optim"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLO8QeVBgU4w"
      },
      "source": [
        "class My_net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(3,9,5)\n",
        "    self.conv2 = torch.nn.Conv2d(9,15,5)\n",
        "    self.conv2drop = torch.nn.Dropout2d(p=0.5)\n",
        "    self.skip_conv2 = torch.nn.Conv2d(3,15,13)\n",
        "    self.line1 = torch.nn.Linear(375,50)\n",
        "    self.line2 = torch.nn.Linear(50,10)\n",
        "    self.resid = torch.nn.Linear(375,10)\n",
        "\n",
        "  def forward(self, signal):\n",
        "    #print(signal.shape)\n",
        "    #singal = torch.nn.BatchNorm2d(3)(signal)\n",
        "    residconv = torch.nn.functional.max_pool2d(self.skip_conv2(signal),2)\n",
        "    #print(residconv.shape)\n",
        "    signal = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(signal),2))\n",
        "    #print(signal.shape)\n",
        "    #signal = torch.nn.BatchNorm2d(9)(signal)\n",
        "    #+residconv\n",
        "    signal = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv2drop(self.conv2(signal)+residconv),2))\n",
        "    #print(signal.shape)\n",
        "    signal = torch.nn.BatchNorm2d(15)(signal)\n",
        "    signal = signal.view(signal.shape[0],-1)\n",
        "    #print(signal.shape)\n",
        "    residual = torch.nn.functional.relu(self.resid(signal))\n",
        "    signal = torch.nn.functional.relu(self.line1(signal))\n",
        "    signal = torch.nn.LayerNorm(50)(signal)\n",
        "    signal = torch.nn.functional.dropout(signal,p=0.5)\n",
        "    #print(signal.shape)\n",
        "    #signal += residual\n",
        "    signal = torch.nn.functional.relu(self.line2(signal)+residual)#\n",
        "    #print(signal.shape)\n",
        "    return torch.nn.functional.log_softmax(signal)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAW4ObHxDAcs"
      },
      "source": [
        "network = My_net()\n",
        "#network_nonnorm = My_net()\n",
        "learnin_rate = 0.003\n",
        "my_momentum = 0.73\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learnin_rate, momentum=my_momentum)\n",
        "#optimizer_non = torch.optim.SGD(network_nonnorm.parameters(), lr=learnin_rate, momentum=my_momentum)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqPTWzpbDbKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c9dfcd-4b8a-4764-edca-49c47e839706"
      },
      "source": [
        "network(torch.rand(7,3,32,32))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7, 15, 10, 10])\n",
            "torch.Size([7, 15, 5, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6629, -2.0013, -2.9132, -0.9024, -2.9132, -2.9132, -2.9132, -2.9132,\n",
              "         -2.7536, -2.9132],\n",
              "        [-1.8334, -3.1022, -2.0370, -2.9981, -2.0131, -2.3706, -3.1022, -2.4380,\n",
              "         -1.5577, -3.1022],\n",
              "        [-0.4330, -3.5589, -3.1119, -3.5589, -3.3443, -3.5589, -3.0431, -3.5589,\n",
              "         -3.5589, -2.5058],\n",
              "        [-2.9746, -2.4574, -2.4722, -2.9746, -1.9210, -2.3913, -2.3718, -2.4787,\n",
              "         -1.3410, -2.9746],\n",
              "        [-2.6549, -2.6549, -2.5544, -2.5433, -1.3541, -2.6549, -2.6549, -2.6549,\n",
              "         -2.3362, -1.9860],\n",
              "        [-2.6855, -1.5664, -2.4855, -3.0198, -2.7329, -2.5465, -1.7544, -2.6792,\n",
              "         -1.8509, -3.0198],\n",
              "        [-2.9012, -2.2948, -1.7166, -2.6253, -1.6346, -2.3272, -2.9012, -2.7517,\n",
              "         -2.0727, -2.9012]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEN-cD7WDwT2"
      },
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #rotator = torchvision.transforms.RandomRotation(degrees=(-10, 10))\n",
        "    #data = rotator(data)\n",
        "    #prob=torch.rand(1)\n",
        "    #if prob[0] > 0.5:\n",
        "    #  data = torchvision.transforms.Grayscale(num_output_channels=3)(data)\n",
        "    my_transforms = torch.nn.ModuleList([#torchvision.transforms.ColorJitter(), #useless cause by default is zero\n",
        "                                         #torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                                         #torchvision.transforms.RandomRotation(degrees=(-10, 10)),\n",
        "                                         #torchvision.transforms.RandomPerspective(),\n",
        "                                         torchvision.transforms.RandomVerticalFlip(p=0.5)#,\n",
        "                                         #torchvision.transforms.GaussianBlur(3, sigma=(0.1, 1.0)),\n",
        "                                         ])\n",
        "    randtran = torchvision.transforms.RandomApply(transforms=my_transforms, p=0.007)\n",
        "    data = randtran(data)\n",
        "\n",
        "    output = network(data)\n",
        "    loss = torch.nn.functional.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKEkw2i3HnSk"
      },
      "source": [
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += torch.nn.functional.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLQULNppKHA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3675659e-193f-4ea7-a338-180084d7bb01"
      },
      "source": [
        "n_epochs=10\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  train(epoch)\n",
        "\n",
        "test()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.448412\n",
            "Train Epoch: 1 [1000/50000 (2%)]\tLoss: 2.394969\n",
            "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 2.430094\n",
            "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 2.456434\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2.221520\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tLoss: 2.283919\n",
            "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 2.322647\n",
            "Train Epoch: 1 [7000/50000 (14%)]\tLoss: 2.266161\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.231339\n",
            "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 2.267982\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2.227967\n",
            "Train Epoch: 1 [11000/50000 (22%)]\tLoss: 2.051410\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 2.160476\n",
            "Train Epoch: 1 [13000/50000 (26%)]\tLoss: 2.104515\n",
            "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 2.048457\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 2.254857\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.144069\n",
            "Train Epoch: 1 [17000/50000 (34%)]\tLoss: 2.175865\n",
            "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 2.157084\n",
            "Train Epoch: 1 [19000/50000 (38%)]\tLoss: 2.019610\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2.073898\n",
            "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 2.062903\n",
            "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 2.055904\n",
            "Train Epoch: 1 [23000/50000 (46%)]\tLoss: 2.187280\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.080262\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tLoss: 2.098132\n",
            "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 2.087102\n",
            "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 2.160450\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 2.079714\n",
            "Train Epoch: 1 [29000/50000 (58%)]\tLoss: 1.954233\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 2.028708\n",
            "Train Epoch: 1 [31000/50000 (62%)]\tLoss: 1.923986\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.906225\n",
            "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 1.939970\n",
            "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 1.983633\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tLoss: 1.884888\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 1.961289\n",
            "Train Epoch: 1 [37000/50000 (74%)]\tLoss: 2.025095\n",
            "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 1.972194\n",
            "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 2.028865\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.859763\n",
            "Train Epoch: 1 [41000/50000 (82%)]\tLoss: 1.906963\n",
            "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 1.876653\n",
            "Train Epoch: 1 [43000/50000 (86%)]\tLoss: 2.015053\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 1.978299\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 1.883961\n",
            "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 2.039372\n",
            "Train Epoch: 1 [47000/50000 (94%)]\tLoss: 1.892393\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.906479\n",
            "Train Epoch: 1 [49000/50000 (98%)]\tLoss: 1.915065\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.785546\n",
            "Train Epoch: 2 [1000/50000 (2%)]\tLoss: 1.942383\n",
            "Train Epoch: 2 [2000/50000 (4%)]\tLoss: 2.016078\n",
            "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 1.827481\n",
            "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 1.772915\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tLoss: 1.909618\n",
            "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 2.005857\n",
            "Train Epoch: 2 [7000/50000 (14%)]\tLoss: 1.954916\n",
            "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.819530\n",
            "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 1.720997\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1.866390\n",
            "Train Epoch: 2 [11000/50000 (22%)]\tLoss: 1.828405\n",
            "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1.952916\n",
            "Train Epoch: 2 [13000/50000 (26%)]\tLoss: 1.938250\n",
            "Train Epoch: 2 [14000/50000 (28%)]\tLoss: 1.823718\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 1.832000\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.919058\n",
            "Train Epoch: 2 [17000/50000 (34%)]\tLoss: 1.826221\n",
            "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 1.756171\n",
            "Train Epoch: 2 [19000/50000 (38%)]\tLoss: 1.802016\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1.725522\n",
            "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 1.895229\n",
            "Train Epoch: 2 [22000/50000 (44%)]\tLoss: 1.942426\n",
            "Train Epoch: 2 [23000/50000 (46%)]\tLoss: 1.863701\n",
            "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.747233\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tLoss: 1.913666\n",
            "Train Epoch: 2 [26000/50000 (52%)]\tLoss: 1.800468\n",
            "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 1.772631\n",
            "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 1.778413\n",
            "Train Epoch: 2 [29000/50000 (58%)]\tLoss: 1.700327\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1.828738\n",
            "Train Epoch: 2 [31000/50000 (62%)]\tLoss: 1.798205\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.847003\n",
            "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 1.606715\n",
            "Train Epoch: 2 [34000/50000 (68%)]\tLoss: 1.771333\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tLoss: 1.740255\n",
            "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.789693\n",
            "Train Epoch: 2 [37000/50000 (74%)]\tLoss: 1.810789\n",
            "Train Epoch: 2 [38000/50000 (76%)]\tLoss: 1.771940\n",
            "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 1.739653\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.820732\n",
            "Train Epoch: 2 [41000/50000 (82%)]\tLoss: 1.866344\n",
            "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 1.862929\n",
            "Train Epoch: 2 [43000/50000 (86%)]\tLoss: 1.832426\n",
            "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 1.921083\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 1.741538\n",
            "Train Epoch: 2 [46000/50000 (92%)]\tLoss: 1.773052\n",
            "Train Epoch: 2 [47000/50000 (94%)]\tLoss: 1.872402\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.682928\n",
            "Train Epoch: 2 [49000/50000 (98%)]\tLoss: 1.700219\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.767807\n",
            "Train Epoch: 3 [1000/50000 (2%)]\tLoss: 1.761826\n",
            "Train Epoch: 3 [2000/50000 (4%)]\tLoss: 1.767946\n",
            "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 1.885951\n",
            "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 1.793566\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tLoss: 1.875900\n",
            "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 1.740485\n",
            "Train Epoch: 3 [7000/50000 (14%)]\tLoss: 1.632352\n",
            "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.831136\n",
            "Train Epoch: 3 [9000/50000 (18%)]\tLoss: 1.659936\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1.779027\n",
            "Train Epoch: 3 [11000/50000 (22%)]\tLoss: 1.754208\n",
            "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1.703914\n",
            "Train Epoch: 3 [13000/50000 (26%)]\tLoss: 1.703126\n",
            "Train Epoch: 3 [14000/50000 (28%)]\tLoss: 1.847009\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tLoss: 1.668989\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.674377\n",
            "Train Epoch: 3 [17000/50000 (34%)]\tLoss: 1.808639\n",
            "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 1.930829\n",
            "Train Epoch: 3 [19000/50000 (38%)]\tLoss: 1.832539\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1.785802\n",
            "Train Epoch: 3 [21000/50000 (42%)]\tLoss: 1.657341\n",
            "Train Epoch: 3 [22000/50000 (44%)]\tLoss: 1.629781\n",
            "Train Epoch: 3 [23000/50000 (46%)]\tLoss: 1.605232\n",
            "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.706579\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tLoss: 1.731522\n",
            "Train Epoch: 3 [26000/50000 (52%)]\tLoss: 1.690308\n",
            "Train Epoch: 3 [27000/50000 (54%)]\tLoss: 1.787709\n",
            "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1.510933\n",
            "Train Epoch: 3 [29000/50000 (58%)]\tLoss: 1.691329\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1.679583\n",
            "Train Epoch: 3 [31000/50000 (62%)]\tLoss: 1.621032\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.856094\n",
            "Train Epoch: 3 [33000/50000 (66%)]\tLoss: 1.624823\n",
            "Train Epoch: 3 [34000/50000 (68%)]\tLoss: 1.746433\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tLoss: 1.754308\n",
            "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1.784386\n",
            "Train Epoch: 3 [37000/50000 (74%)]\tLoss: 1.572929\n",
            "Train Epoch: 3 [38000/50000 (76%)]\tLoss: 1.679831\n",
            "Train Epoch: 3 [39000/50000 (78%)]\tLoss: 1.644976\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.639442\n",
            "Train Epoch: 3 [41000/50000 (82%)]\tLoss: 1.673228\n",
            "Train Epoch: 3 [42000/50000 (84%)]\tLoss: 1.690253\n",
            "Train Epoch: 3 [43000/50000 (86%)]\tLoss: 1.500366\n",
            "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 1.748197\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tLoss: 1.592151\n",
            "Train Epoch: 3 [46000/50000 (92%)]\tLoss: 1.620418\n",
            "Train Epoch: 3 [47000/50000 (94%)]\tLoss: 1.588549\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.624706\n",
            "Train Epoch: 3 [49000/50000 (98%)]\tLoss: 1.520128\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.542055\n",
            "Train Epoch: 4 [1000/50000 (2%)]\tLoss: 1.558161\n",
            "Train Epoch: 4 [2000/50000 (4%)]\tLoss: 1.755829\n",
            "Train Epoch: 4 [3000/50000 (6%)]\tLoss: 1.740701\n",
            "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 1.529566\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tLoss: 1.745063\n",
            "Train Epoch: 4 [6000/50000 (12%)]\tLoss: 1.509953\n",
            "Train Epoch: 4 [7000/50000 (14%)]\tLoss: 1.881325\n",
            "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.606225\n",
            "Train Epoch: 4 [9000/50000 (18%)]\tLoss: 1.884773\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1.788723\n",
            "Train Epoch: 4 [11000/50000 (22%)]\tLoss: 1.594018\n",
            "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1.753040\n",
            "Train Epoch: 4 [13000/50000 (26%)]\tLoss: 1.733201\n",
            "Train Epoch: 4 [14000/50000 (28%)]\tLoss: 1.590432\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tLoss: 1.668097\n",
            "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.767153\n",
            "Train Epoch: 4 [17000/50000 (34%)]\tLoss: 1.493090\n",
            "Train Epoch: 4 [18000/50000 (36%)]\tLoss: 1.843580\n",
            "Train Epoch: 4 [19000/50000 (38%)]\tLoss: 1.643944\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1.496609\n",
            "Train Epoch: 4 [21000/50000 (42%)]\tLoss: 1.829520\n",
            "Train Epoch: 4 [22000/50000 (44%)]\tLoss: 1.579742\n",
            "Train Epoch: 4 [23000/50000 (46%)]\tLoss: 1.731208\n",
            "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.865539\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tLoss: 1.600527\n",
            "Train Epoch: 4 [26000/50000 (52%)]\tLoss: 1.737171\n",
            "Train Epoch: 4 [27000/50000 (54%)]\tLoss: 1.646121\n",
            "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 1.562415\n",
            "Train Epoch: 4 [29000/50000 (58%)]\tLoss: 1.568912\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1.656891\n",
            "Train Epoch: 4 [31000/50000 (62%)]\tLoss: 1.780391\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.664431\n",
            "Train Epoch: 4 [33000/50000 (66%)]\tLoss: 1.860408\n",
            "Train Epoch: 4 [34000/50000 (68%)]\tLoss: 1.831570\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tLoss: 1.671840\n",
            "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1.700747\n",
            "Train Epoch: 4 [37000/50000 (74%)]\tLoss: 1.593020\n",
            "Train Epoch: 4 [38000/50000 (76%)]\tLoss: 1.669205\n",
            "Train Epoch: 4 [39000/50000 (78%)]\tLoss: 1.753469\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.657756\n",
            "Train Epoch: 4 [41000/50000 (82%)]\tLoss: 1.754976\n",
            "Train Epoch: 4 [42000/50000 (84%)]\tLoss: 1.519798\n",
            "Train Epoch: 4 [43000/50000 (86%)]\tLoss: 1.564132\n",
            "Train Epoch: 4 [44000/50000 (88%)]\tLoss: 1.480241\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tLoss: 1.656700\n",
            "Train Epoch: 4 [46000/50000 (92%)]\tLoss: 1.471107\n",
            "Train Epoch: 4 [47000/50000 (94%)]\tLoss: 1.622966\n",
            "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.659633\n",
            "Train Epoch: 4 [49000/50000 (98%)]\tLoss: 1.579190\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.669148\n",
            "Train Epoch: 5 [1000/50000 (2%)]\tLoss: 1.671773\n",
            "Train Epoch: 5 [2000/50000 (4%)]\tLoss: 1.624768\n",
            "Train Epoch: 5 [3000/50000 (6%)]\tLoss: 1.541964\n",
            "Train Epoch: 5 [4000/50000 (8%)]\tLoss: 1.698896\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tLoss: 1.605601\n",
            "Train Epoch: 5 [6000/50000 (12%)]\tLoss: 1.739894\n",
            "Train Epoch: 5 [7000/50000 (14%)]\tLoss: 1.778412\n",
            "Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.627032\n",
            "Train Epoch: 5 [9000/50000 (18%)]\tLoss: 1.478467\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1.660170\n",
            "Train Epoch: 5 [11000/50000 (22%)]\tLoss: 1.697705\n",
            "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1.704717\n",
            "Train Epoch: 5 [13000/50000 (26%)]\tLoss: 1.634497\n",
            "Train Epoch: 5 [14000/50000 (28%)]\tLoss: 1.666447\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tLoss: 1.380042\n",
            "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.532845\n",
            "Train Epoch: 5 [17000/50000 (34%)]\tLoss: 1.537051\n",
            "Train Epoch: 5 [18000/50000 (36%)]\tLoss: 1.614359\n",
            "Train Epoch: 5 [19000/50000 (38%)]\tLoss: 1.685515\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1.647988\n",
            "Train Epoch: 5 [21000/50000 (42%)]\tLoss: 1.574351\n",
            "Train Epoch: 5 [22000/50000 (44%)]\tLoss: 1.682898\n",
            "Train Epoch: 5 [23000/50000 (46%)]\tLoss: 1.639673\n",
            "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1.525827\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tLoss: 1.512585\n",
            "Train Epoch: 5 [26000/50000 (52%)]\tLoss: 1.626576\n",
            "Train Epoch: 5 [27000/50000 (54%)]\tLoss: 1.777864\n",
            "Train Epoch: 5 [28000/50000 (56%)]\tLoss: 1.622579\n",
            "Train Epoch: 5 [29000/50000 (58%)]\tLoss: 1.681220\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1.481030\n",
            "Train Epoch: 5 [31000/50000 (62%)]\tLoss: 1.745165\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.661226\n",
            "Train Epoch: 5 [33000/50000 (66%)]\tLoss: 1.637947\n",
            "Train Epoch: 5 [34000/50000 (68%)]\tLoss: 1.812582\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tLoss: 1.522928\n",
            "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1.709442\n",
            "Train Epoch: 5 [37000/50000 (74%)]\tLoss: 1.440868\n",
            "Train Epoch: 5 [38000/50000 (76%)]\tLoss: 1.822450\n",
            "Train Epoch: 5 [39000/50000 (78%)]\tLoss: 1.570865\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.693999\n",
            "Train Epoch: 5 [41000/50000 (82%)]\tLoss: 1.578006\n",
            "Train Epoch: 5 [42000/50000 (84%)]\tLoss: 1.699755\n",
            "Train Epoch: 5 [43000/50000 (86%)]\tLoss: 1.561258\n",
            "Train Epoch: 5 [44000/50000 (88%)]\tLoss: 1.663215\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tLoss: 1.576291\n",
            "Train Epoch: 5 [46000/50000 (92%)]\tLoss: 1.627645\n",
            "Train Epoch: 5 [47000/50000 (94%)]\tLoss: 1.731149\n",
            "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.573545\n",
            "Train Epoch: 5 [49000/50000 (98%)]\tLoss: 1.714391\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.656446\n",
            "Train Epoch: 6 [1000/50000 (2%)]\tLoss: 1.653017\n",
            "Train Epoch: 6 [2000/50000 (4%)]\tLoss: 1.576624\n",
            "Train Epoch: 6 [3000/50000 (6%)]\tLoss: 1.533401\n",
            "Train Epoch: 6 [4000/50000 (8%)]\tLoss: 1.618211\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tLoss: 1.720620\n",
            "Train Epoch: 6 [6000/50000 (12%)]\tLoss: 1.639751\n",
            "Train Epoch: 6 [7000/50000 (14%)]\tLoss: 1.669694\n",
            "Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.606063\n",
            "Train Epoch: 6 [9000/50000 (18%)]\tLoss: 1.545627\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tLoss: 1.423932\n",
            "Train Epoch: 6 [11000/50000 (22%)]\tLoss: 1.659143\n",
            "Train Epoch: 6 [12000/50000 (24%)]\tLoss: 1.567898\n",
            "Train Epoch: 6 [13000/50000 (26%)]\tLoss: 1.876765\n",
            "Train Epoch: 6 [14000/50000 (28%)]\tLoss: 1.305103\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tLoss: 1.458009\n",
            "Train Epoch: 6 [16000/50000 (32%)]\tLoss: 1.568611\n",
            "Train Epoch: 6 [17000/50000 (34%)]\tLoss: 1.670484\n",
            "Train Epoch: 6 [18000/50000 (36%)]\tLoss: 1.582976\n",
            "Train Epoch: 6 [19000/50000 (38%)]\tLoss: 1.663660\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1.650503\n",
            "Train Epoch: 6 [21000/50000 (42%)]\tLoss: 1.552611\n",
            "Train Epoch: 6 [22000/50000 (44%)]\tLoss: 1.462403\n",
            "Train Epoch: 6 [23000/50000 (46%)]\tLoss: 1.521440\n",
            "Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.864518\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tLoss: 1.480020\n",
            "Train Epoch: 6 [26000/50000 (52%)]\tLoss: 1.413131\n",
            "Train Epoch: 6 [27000/50000 (54%)]\tLoss: 1.643739\n",
            "Train Epoch: 6 [28000/50000 (56%)]\tLoss: 1.570786\n",
            "Train Epoch: 6 [29000/50000 (58%)]\tLoss: 1.558805\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tLoss: 1.628648\n",
            "Train Epoch: 6 [31000/50000 (62%)]\tLoss: 1.689875\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.667539\n",
            "Train Epoch: 6 [33000/50000 (66%)]\tLoss: 1.639561\n",
            "Train Epoch: 6 [34000/50000 (68%)]\tLoss: 1.646745\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tLoss: 1.583318\n",
            "Train Epoch: 6 [36000/50000 (72%)]\tLoss: 1.535815\n",
            "Train Epoch: 6 [37000/50000 (74%)]\tLoss: 1.418648\n",
            "Train Epoch: 6 [38000/50000 (76%)]\tLoss: 1.637291\n",
            "Train Epoch: 6 [39000/50000 (78%)]\tLoss: 1.630659\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.637982\n",
            "Train Epoch: 6 [41000/50000 (82%)]\tLoss: 1.746563\n",
            "Train Epoch: 6 [42000/50000 (84%)]\tLoss: 1.657881\n",
            "Train Epoch: 6 [43000/50000 (86%)]\tLoss: 1.653183\n",
            "Train Epoch: 6 [44000/50000 (88%)]\tLoss: 1.787769\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tLoss: 1.535826\n",
            "Train Epoch: 6 [46000/50000 (92%)]\tLoss: 1.626010\n",
            "Train Epoch: 6 [47000/50000 (94%)]\tLoss: 1.634336\n",
            "Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.580462\n",
            "Train Epoch: 6 [49000/50000 (98%)]\tLoss: 1.745723\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.592947\n",
            "Train Epoch: 7 [1000/50000 (2%)]\tLoss: 1.681115\n",
            "Train Epoch: 7 [2000/50000 (4%)]\tLoss: 1.606686\n",
            "Train Epoch: 7 [3000/50000 (6%)]\tLoss: 1.621435\n",
            "Train Epoch: 7 [4000/50000 (8%)]\tLoss: 1.277059\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tLoss: 1.740470\n",
            "Train Epoch: 7 [6000/50000 (12%)]\tLoss: 1.447067\n",
            "Train Epoch: 7 [7000/50000 (14%)]\tLoss: 1.628535\n",
            "Train Epoch: 7 [8000/50000 (16%)]\tLoss: 1.487501\n",
            "Train Epoch: 7 [9000/50000 (18%)]\tLoss: 1.572002\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tLoss: 1.643422\n",
            "Train Epoch: 7 [11000/50000 (22%)]\tLoss: 1.694073\n",
            "Train Epoch: 7 [12000/50000 (24%)]\tLoss: 1.586626\n",
            "Train Epoch: 7 [13000/50000 (26%)]\tLoss: 1.831971\n",
            "Train Epoch: 7 [14000/50000 (28%)]\tLoss: 1.529715\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tLoss: 1.458684\n",
            "Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.745229\n",
            "Train Epoch: 7 [17000/50000 (34%)]\tLoss: 1.596379\n",
            "Train Epoch: 7 [18000/50000 (36%)]\tLoss: 1.638117\n",
            "Train Epoch: 7 [19000/50000 (38%)]\tLoss: 1.642874\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1.594536\n",
            "Train Epoch: 7 [21000/50000 (42%)]\tLoss: 1.612453\n",
            "Train Epoch: 7 [22000/50000 (44%)]\tLoss: 1.501066\n",
            "Train Epoch: 7 [23000/50000 (46%)]\tLoss: 1.705792\n",
            "Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.554876\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tLoss: 1.479654\n",
            "Train Epoch: 7 [26000/50000 (52%)]\tLoss: 1.520110\n",
            "Train Epoch: 7 [27000/50000 (54%)]\tLoss: 1.586873\n",
            "Train Epoch: 7 [28000/50000 (56%)]\tLoss: 1.463347\n",
            "Train Epoch: 7 [29000/50000 (58%)]\tLoss: 1.629911\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tLoss: 1.451828\n",
            "Train Epoch: 7 [31000/50000 (62%)]\tLoss: 1.494140\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.435243\n",
            "Train Epoch: 7 [33000/50000 (66%)]\tLoss: 1.688276\n",
            "Train Epoch: 7 [34000/50000 (68%)]\tLoss: 1.548775\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tLoss: 1.285837\n",
            "Train Epoch: 7 [36000/50000 (72%)]\tLoss: 1.557563\n",
            "Train Epoch: 7 [37000/50000 (74%)]\tLoss: 1.656653\n",
            "Train Epoch: 7 [38000/50000 (76%)]\tLoss: 1.563723\n",
            "Train Epoch: 7 [39000/50000 (78%)]\tLoss: 1.642385\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.730899\n",
            "Train Epoch: 7 [41000/50000 (82%)]\tLoss: 1.536556\n",
            "Train Epoch: 7 [42000/50000 (84%)]\tLoss: 1.656620\n",
            "Train Epoch: 7 [43000/50000 (86%)]\tLoss: 1.551193\n",
            "Train Epoch: 7 [44000/50000 (88%)]\tLoss: 1.726758\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tLoss: 1.619536\n",
            "Train Epoch: 7 [46000/50000 (92%)]\tLoss: 1.494341\n",
            "Train Epoch: 7 [47000/50000 (94%)]\tLoss: 1.657043\n",
            "Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.558878\n",
            "Train Epoch: 7 [49000/50000 (98%)]\tLoss: 1.605793\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.465586\n",
            "Train Epoch: 8 [1000/50000 (2%)]\tLoss: 1.636256\n",
            "Train Epoch: 8 [2000/50000 (4%)]\tLoss: 1.571896\n",
            "Train Epoch: 8 [3000/50000 (6%)]\tLoss: 1.606685\n",
            "Train Epoch: 8 [4000/50000 (8%)]\tLoss: 1.500088\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tLoss: 1.513994\n",
            "Train Epoch: 8 [6000/50000 (12%)]\tLoss: 1.416189\n",
            "Train Epoch: 8 [7000/50000 (14%)]\tLoss: 1.668850\n",
            "Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.533605\n",
            "Train Epoch: 8 [9000/50000 (18%)]\tLoss: 1.564705\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tLoss: 1.479518\n",
            "Train Epoch: 8 [11000/50000 (22%)]\tLoss: 1.651600\n",
            "Train Epoch: 8 [12000/50000 (24%)]\tLoss: 1.546934\n",
            "Train Epoch: 8 [13000/50000 (26%)]\tLoss: 1.508336\n",
            "Train Epoch: 8 [14000/50000 (28%)]\tLoss: 1.573173\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tLoss: 1.585440\n",
            "Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.465799\n",
            "Train Epoch: 8 [17000/50000 (34%)]\tLoss: 1.554007\n",
            "Train Epoch: 8 [18000/50000 (36%)]\tLoss: 1.440789\n",
            "Train Epoch: 8 [19000/50000 (38%)]\tLoss: 1.641184\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1.496796\n",
            "Train Epoch: 8 [21000/50000 (42%)]\tLoss: 1.655143\n",
            "Train Epoch: 8 [22000/50000 (44%)]\tLoss: 1.595402\n",
            "Train Epoch: 8 [23000/50000 (46%)]\tLoss: 1.572195\n",
            "Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.487823\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tLoss: 1.454944\n",
            "Train Epoch: 8 [26000/50000 (52%)]\tLoss: 1.628560\n",
            "Train Epoch: 8 [27000/50000 (54%)]\tLoss: 1.734252\n",
            "Train Epoch: 8 [28000/50000 (56%)]\tLoss: 1.640365\n",
            "Train Epoch: 8 [29000/50000 (58%)]\tLoss: 1.620853\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tLoss: 1.606065\n",
            "Train Epoch: 8 [31000/50000 (62%)]\tLoss: 1.583150\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.479061\n",
            "Train Epoch: 8 [33000/50000 (66%)]\tLoss: 1.571228\n",
            "Train Epoch: 8 [34000/50000 (68%)]\tLoss: 1.627640\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tLoss: 1.451115\n",
            "Train Epoch: 8 [36000/50000 (72%)]\tLoss: 1.679438\n",
            "Train Epoch: 8 [37000/50000 (74%)]\tLoss: 1.523678\n",
            "Train Epoch: 8 [38000/50000 (76%)]\tLoss: 1.591385\n",
            "Train Epoch: 8 [39000/50000 (78%)]\tLoss: 1.653046\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.613981\n",
            "Train Epoch: 8 [41000/50000 (82%)]\tLoss: 1.605416\n",
            "Train Epoch: 8 [42000/50000 (84%)]\tLoss: 1.713818\n",
            "Train Epoch: 8 [43000/50000 (86%)]\tLoss: 1.492411\n",
            "Train Epoch: 8 [44000/50000 (88%)]\tLoss: 1.523060\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tLoss: 1.313325\n",
            "Train Epoch: 8 [46000/50000 (92%)]\tLoss: 1.628390\n",
            "Train Epoch: 8 [47000/50000 (94%)]\tLoss: 1.571855\n",
            "Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.463551\n",
            "Train Epoch: 8 [49000/50000 (98%)]\tLoss: 1.519886\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.516747\n",
            "Train Epoch: 9 [1000/50000 (2%)]\tLoss: 1.545703\n",
            "Train Epoch: 9 [2000/50000 (4%)]\tLoss: 1.574131\n",
            "Train Epoch: 9 [3000/50000 (6%)]\tLoss: 1.440644\n",
            "Train Epoch: 9 [4000/50000 (8%)]\tLoss: 1.623299\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tLoss: 1.682203\n",
            "Train Epoch: 9 [6000/50000 (12%)]\tLoss: 1.589362\n",
            "Train Epoch: 9 [7000/50000 (14%)]\tLoss: 1.513264\n",
            "Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.574739\n",
            "Train Epoch: 9 [9000/50000 (18%)]\tLoss: 1.553538\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tLoss: 1.666622\n",
            "Train Epoch: 9 [11000/50000 (22%)]\tLoss: 1.461392\n",
            "Train Epoch: 9 [12000/50000 (24%)]\tLoss: 1.525233\n",
            "Train Epoch: 9 [13000/50000 (26%)]\tLoss: 1.558903\n",
            "Train Epoch: 9 [14000/50000 (28%)]\tLoss: 1.589213\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tLoss: 1.582458\n",
            "Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.155553\n",
            "Train Epoch: 9 [17000/50000 (34%)]\tLoss: 1.577278\n",
            "Train Epoch: 9 [18000/50000 (36%)]\tLoss: 1.412766\n",
            "Train Epoch: 9 [19000/50000 (38%)]\tLoss: 1.732817\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1.602357\n",
            "Train Epoch: 9 [21000/50000 (42%)]\tLoss: 1.424589\n",
            "Train Epoch: 9 [22000/50000 (44%)]\tLoss: 1.763936\n",
            "Train Epoch: 9 [23000/50000 (46%)]\tLoss: 1.463212\n",
            "Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.543623\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tLoss: 1.626259\n",
            "Train Epoch: 9 [26000/50000 (52%)]\tLoss: 1.462004\n",
            "Train Epoch: 9 [27000/50000 (54%)]\tLoss: 1.476778\n",
            "Train Epoch: 9 [28000/50000 (56%)]\tLoss: 1.535796\n",
            "Train Epoch: 9 [29000/50000 (58%)]\tLoss: 1.685038\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tLoss: 2.265131\n",
            "Train Epoch: 9 [31000/50000 (62%)]\tLoss: 1.649919\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.546098\n",
            "Train Epoch: 9 [33000/50000 (66%)]\tLoss: 1.705760\n",
            "Train Epoch: 9 [34000/50000 (68%)]\tLoss: 1.722487\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tLoss: 1.706947\n",
            "Train Epoch: 9 [36000/50000 (72%)]\tLoss: 1.606831\n",
            "Train Epoch: 9 [37000/50000 (74%)]\tLoss: 1.786383\n",
            "Train Epoch: 9 [38000/50000 (76%)]\tLoss: 1.546714\n",
            "Train Epoch: 9 [39000/50000 (78%)]\tLoss: 1.425993\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.575347\n",
            "Train Epoch: 9 [41000/50000 (82%)]\tLoss: 1.579861\n",
            "Train Epoch: 9 [42000/50000 (84%)]\tLoss: 1.606916\n",
            "Train Epoch: 9 [43000/50000 (86%)]\tLoss: 1.539048\n",
            "Train Epoch: 9 [44000/50000 (88%)]\tLoss: 1.386269\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tLoss: 1.479575\n",
            "Train Epoch: 9 [46000/50000 (92%)]\tLoss: 1.526952\n",
            "Train Epoch: 9 [47000/50000 (94%)]\tLoss: 1.672823\n",
            "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.496942\n",
            "Train Epoch: 9 [49000/50000 (98%)]\tLoss: 1.492291\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.561823\n",
            "Train Epoch: 10 [1000/50000 (2%)]\tLoss: 1.490413\n",
            "Train Epoch: 10 [2000/50000 (4%)]\tLoss: 1.601070\n",
            "Train Epoch: 10 [3000/50000 (6%)]\tLoss: 1.582891\n",
            "Train Epoch: 10 [4000/50000 (8%)]\tLoss: 1.556991\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tLoss: 1.485192\n",
            "Train Epoch: 10 [6000/50000 (12%)]\tLoss: 1.535879\n",
            "Train Epoch: 10 [7000/50000 (14%)]\tLoss: 1.652585\n",
            "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.555881\n",
            "Train Epoch: 10 [9000/50000 (18%)]\tLoss: 1.667315\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tLoss: 1.567493\n",
            "Train Epoch: 10 [11000/50000 (22%)]\tLoss: 1.537779\n",
            "Train Epoch: 10 [12000/50000 (24%)]\tLoss: 1.487950\n",
            "Train Epoch: 10 [13000/50000 (26%)]\tLoss: 1.377236\n",
            "Train Epoch: 10 [14000/50000 (28%)]\tLoss: 1.652390\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tLoss: 1.648023\n",
            "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.618370\n",
            "Train Epoch: 10 [17000/50000 (34%)]\tLoss: 1.406845\n",
            "Train Epoch: 10 [18000/50000 (36%)]\tLoss: 1.418796\n",
            "Train Epoch: 10 [19000/50000 (38%)]\tLoss: 1.421766\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1.643961\n",
            "Train Epoch: 10 [21000/50000 (42%)]\tLoss: 1.401032\n",
            "Train Epoch: 10 [22000/50000 (44%)]\tLoss: 1.713562\n",
            "Train Epoch: 10 [23000/50000 (46%)]\tLoss: 1.516449\n",
            "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.536250\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tLoss: 1.612788\n",
            "Train Epoch: 10 [26000/50000 (52%)]\tLoss: 1.525904\n",
            "Train Epoch: 10 [27000/50000 (54%)]\tLoss: 1.512916\n",
            "Train Epoch: 10 [28000/50000 (56%)]\tLoss: 1.511665\n",
            "Train Epoch: 10 [29000/50000 (58%)]\tLoss: 1.617568\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1.493884\n",
            "Train Epoch: 10 [31000/50000 (62%)]\tLoss: 1.470086\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.694661\n",
            "Train Epoch: 10 [33000/50000 (66%)]\tLoss: 1.565406\n",
            "Train Epoch: 10 [34000/50000 (68%)]\tLoss: 1.450449\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tLoss: 1.599557\n",
            "Train Epoch: 10 [36000/50000 (72%)]\tLoss: 1.534238\n",
            "Train Epoch: 10 [37000/50000 (74%)]\tLoss: 1.614017\n",
            "Train Epoch: 10 [38000/50000 (76%)]\tLoss: 1.777944\n",
            "Train Epoch: 10 [39000/50000 (78%)]\tLoss: 1.637131\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.424326\n",
            "Train Epoch: 10 [41000/50000 (82%)]\tLoss: 1.663805\n",
            "Train Epoch: 10 [42000/50000 (84%)]\tLoss: 1.683031\n",
            "Train Epoch: 10 [43000/50000 (86%)]\tLoss: 1.641391\n",
            "Train Epoch: 10 [44000/50000 (88%)]\tLoss: 1.808654\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tLoss: 1.571475\n",
            "Train Epoch: 10 [46000/50000 (92%)]\tLoss: 1.594779\n",
            "Train Epoch: 10 [47000/50000 (94%)]\tLoss: 1.605220\n",
            "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.611919\n",
            "Train Epoch: 10 [49000/50000 (98%)]\tLoss: 1.486942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 1.3538, Accuracy: 5215/10000 (52%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIII3lM2KIma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f504937-aea7-401d-bb04-a4f9fc6a5b54"
      },
      "source": [
        "test()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 1.3536, Accuracy: 5231/10000 (52%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxU6qCgTb3Gc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}